{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\My\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\My\n",
      "[nltk_data]     PC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import bs4 as bs\n",
    "import urllib.request\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.cluster.util import cosine_distance\n",
    "import networkx as nx\n",
    "import re \n",
    "import heapq\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def userinput():\n",
    "    inp_url=input(\"Enter a url: \")\n",
    "    return inp_url\n",
    "\n",
    "\"\"\"function - checking validity of input (This program is dealing with wikipedia pages only. \n",
    "Links to other websites are considered invalid )\"\"\"\n",
    "def validity_input(inp_url):\n",
    "    if inp_url[0:30]!='https://en.wikipedia.org/wiki/':\n",
    "        return False\n",
    "    else:\n",
    "         return True\n",
    "\n",
    "def get_data(inp_url):\n",
    "    #storing data scraped from wikipedia page \n",
    "    scraped_data = urllib.request.urlopen(inp_url)\n",
    "    \n",
    "    #reading the scraped data - type(wikiarticle) = bytes\n",
    "    wikiarticle = scraped_data.read() \n",
    "    \n",
    "    #converting the data into a BeautifulSoup object. type(parsed_article)=bs4.BeautifulSoup\n",
    "    parsed_article = bs.BeautifulSoup(wikiarticle,'lxml')\n",
    "    \n",
    "    \"\"\"storing all the text in the webpage, which is enclosed within <p> and </p> tags. type(paragraphs)=bs4.element.ResultSet\n",
    "    ResultSet is iterable, supports indexing. Basically functions as a list in this program\"\"\"\n",
    "    paragraphs = parsed_article.find_all('p') \n",
    "\n",
    "    wikiarticle_text = \"\"\n",
    "    \n",
    "    #storing all text from the webpage in a string\n",
    "    for p in paragraphs:  \n",
    "        wikiarticle_text += p.text\n",
    "    \n",
    "    return wikiarticle_text\n",
    "        \n",
    "def format_data(wikiarticle_text):\n",
    "    #replacing references - numbers enclosed in square brackets - with spaces\n",
    "    wikiarticle_text = re.sub(r'\\[[0-9]*\\]', ' ', wikiarticle_text)\n",
    "    \n",
    "    #replacing multiple spaces with single space\n",
    "    wikiarticle_text = re.sub(r'\\s+', ' ', wikiarticle_text)\n",
    "    \n",
    "    return wikiarticle_text\n",
    "def sentence_similarity(sent1, sent2, stopwords=None):\n",
    "    if stopwords is None:\n",
    "        stopwords = []\n",
    " \n",
    "    sent1 = [w.lower() for w in sent1]\n",
    "    sent2 = [w.lower() for w in sent2]\n",
    " \n",
    "    all_words = list(set(sent1 + sent2))\n",
    " \n",
    "    vector1 = [0] * len(all_words)\n",
    "    vector2 = [0] * len(all_words)\n",
    " \n",
    "    # build the vector for the first sentence\n",
    "    for w in sent1:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector1[all_words.index(w)] += 1\n",
    " \n",
    "    # build the vector for the second sentence\n",
    "    for w in sent2:\n",
    "        if w in stopwords:\n",
    "            continue\n",
    "        vector2[all_words.index(w)] += 1\n",
    " \n",
    "    return 1 - cosine_distance(vector1, vector2)\n",
    "\n",
    "def build_similarity_matrix(sentences, stop_words):\n",
    "    # Create an empty similarity matrix\n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences))) #Creates an empty 2x2 matrix filled with zeros\n",
    " \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 == idx2: #ignore if both are same sentences\n",
    "                continue \n",
    "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
    "\n",
    "    return similarity_matrix\n",
    "def get_summary(full_text,n):\n",
    "    sentence_list = nltk.sent_tokenize(full_text)\n",
    "    new_sentence_list=[]\n",
    "    for sentence in sentence_list:\n",
    "        new_sentence_list.append(re.sub(\"\\?\", '', sentence))\n",
    "    #Usually the first sentece is very important in a wikipedia article. So making sure that it is included.\n",
    "    mandate = new_sentence_list.pop(0)\n",
    "    sentence_similarity_martix = build_similarity_matrix(new_sentence_list, stop_words)\n",
    "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix) #Creates graph\n",
    "    scores = nx.pagerank(sentence_similarity_graph) #Scores the graph using PageRank\n",
    "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(new_sentence_list)), reverse=True)    \n",
    "    summarize_text = []\n",
    "    for i in range(n):\n",
    "        summarize_text.append(\"\".join(ranked_sentence[i][1]))\n",
    "    summary= (\" \".join(summarize_text))\n",
    "    summary = mandate + \" \" + summary\n",
    "    return summary\n",
    "\n",
    "#calling all the above functions\n",
    "def call():\n",
    "    print(\"Enter URL for a Wikipedia (strictly) Article:\")\n",
    "    inp_url=userinput()\n",
    "    print(\"Enter the number of sentences you want your summary to be:\")\n",
    "    n = int(input())\n",
    "    print()\n",
    "    while validity_input(inp_url)!=True:\n",
    "        print(\"Invalid url. Please enter the url of any wikipedia page.\")\n",
    "        inp_url=userinput()\n",
    "    article = get_data(inp_url)\n",
    "    article = format_data(article)\n",
    "    summary = get_summary(article,n)\n",
    "    print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter URL for a Wikipedia (strictly) Article:\n",
      "Enter a url: https://en.wikipedia.org/wiki/Imputation_(statistics)\n",
      "Enter the number of sentences you want your summary to be:\n",
      "5\n",
      "\n",
      "In statistics, imputation is the process of replacing missing data with substituted values. In order to deal with the problem of increased noise due to imputation, Rubin (1987) developed a method for averaging the outcomes across multiple imputed data sets to account for this. Because of the incomplete N values at some points in time, while still maintaining complete case comparison for other parameters, pairwise deletion can introduce impossible mathematical situations such as correlations that are over 100%. The problem is that the imputed data do not have an error term included in their estimation, thus the estimates fit perfectly along the regression line without any residual variance. After imputation, the data is treated as if they were the actual real values in single imputation. A once-common method of imputation was hot-deck imputation where a missing value was imputed from a randomly selected similar record.\n"
     ]
    }
   ],
   "source": [
    "call()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
